#This module "Experiment_Framework" has functions which are used in conducting numerical experiments. 
#These functions include:
#1. product_diff_list
#2. question_extractor
#3. enum_two_step
#4. enum_two_step_opt
#5. quantile_test_enum_data
#6. MSE_det_test ##THIS HAS BEEN REWRITTEN!


--------------------------------------------------------------------------------------------------------------------------------------------------------------
*************************************************************************************************************************************************************
--------------------------------------------------------------------------------------------------------------------------------------------------------------

import numpy as np
import pandas as pd
import itertools
import matplotlib.pyplot as plt
import random
import seaborn as sns
from itertools import combinations
import sklearn.datasets

import sys
sys.path.append(r'C:\Users\wsfishe\Desktop\PreferenceElicitationCode')
from Questionnaire_Procedure import *
from Baseline_Functions_Definitions import g_fun_linear_regression

#From Questionnaire_Procedure we are importing:

#PACKAGES:
# gurobipy
# numpy
# scipy.integrate
# math

#FUNCTIONS:
# z_expectation_variance (originally from Baseline_Functions_Definitions)
# g_fun (originally from Baseline_Functions_Definitions)
# moment_matching_update
# g_opt
# two_stage_g_opt
# two_step_g_acq

#!!NO LONGER USING!!
#VARIABLES:
#mu_log_coeff = 0.03596804494858049
#Sig_log_coeff = -0.020785433813507195

--------------------------------------------------------------------------------------------------------------------------------------------------------------
*************************************************************************************************************************************************************
--------------------------------------------------------------------------------------------------------------------------------------------------------------

#Define a set that has all the differences between binary products

def product_diff_list(n):
    #n: the number of attributes of the products
    
    #Example of itertools.product:
    #itertools.product(range(2), repeat = 3) --> 000 001 010 011 100 101 110 111
    p_d_l = list(itertools.product([-1.0,0.0,1.0],repeat = n))
    
    #Return the index of the tuple with all 0s.
    zero_index = p_d_l.index(tuple([0]*n))

    #Note that at this point, product_diff_list contains some redundant information. Due to
    #the symmetry of the one-step and two-step acquisition function in terms of question mean, question pairs such as 
    #(-1,-1,-1,...,-1) and (1,1,1,...,1) (i.e. negative multiples) will evaluate as the same under the one-step and two-step
    #acquisition functions. Due to the structure of product_diff_list, we can remove every question pair before and including
    #the question pair with all zero entries in order to remove this redundant information.
    for i in range(0,zero_index + 1):
        p_d_l.pop(0)
    
    p_d_l = [np.array(a) for a in p_d_l]
    return p_d_l
    
--------------------------------------------------------------------------------------------------------------------------------------------------------------
*************************************************************************************************************************************************************
--------------------------------------------------------------------------------------------------------------------------------------------------------------

#Given a trinary vector of 0, 1, and -1, find two binary products whose difference is the trinary vector.
def question_extractor(prod):
    #prod: This is a trinary vector of 0, 1, and -1 that represents the difference between two products, which
    #are represented by binary vectors.
    
    x = [0]*len(prod)
    y = [0]*len(prod)
    for i in range(0,len(prod)):
        if prod[i] == 1.0:
            x[i] = 1.0
            y[i] = 0.0
        if prod[i] == 0.0:
            x[i] = 0.0
            y[i] = 0.0
        if prod[i] == -1.0:
            x[i] = 0.0
            y[i] = 1.0
    return x,y

--------------------------------------------------------------------------------------------------------------------------------------------------------------
*************************************************************************************************************************************************************
--------------------------------------------------------------------------------------------------------------------------------------------------------------

#A function which returns a list of all the enumerated two-step acquisition values of a given product set. We use the exact
#two-step g function. We return the prod_diff_set in case there was sampling done and we have interest in the sampled
#question pairs. We also return a list of all the first stage and two second stage questions.
def enum_two_step(mu_vec, Sig_mat, mu_log_coeff, Sig_log_coeff, prod_diff_set, prod_samp_num = 0):
    #mu_vec: expectation of prior on beta
    #Sig_mat: covariance matrix of prior on beta
    #mu_log_coeff and Sig_log_coeff: coefficients that are used in the optimization prblem.
    #prod_diff_set: set of question pairs we are enumerating over. This should be created by using product_diff_list.
    #prod_samp_num: Should be a positive integer. Used in obtaining a random sample of product pairs if needed.
    
    #Sample a number of product pairs from the product pairs list, if needed in the case where
    #there are a large number of attributes. Will possibly need a seed for random.sample()
    if prod_samp_num>0:
        prod_diff_set = random.sample(prod_diff_set,prod_samp_num)
    
    #define a list to store enumerated two-step values, along with a list to save the first stage and two second stage
    #questions.
    prod_diff_len = len(prod_diff_set)
    two_step_g_val = [0]*prod_diff_len
    first_stage_second_stage_questions = [0]*prod_diff_len
    
    #calculate two-step values for all question pairs
    for i in range(0, prod_diff_len):
        x_0,y_0 = question_extractor(prod_diff_set[i])
        two_step = two_step_g_acq(mu_vec,Sig_mat,mu_log_coeff,Sig_log_coeff,x_0,y_0)
        two_step_g_val[i] = two_step[0]
        first_stage_second_stage_questions[i] = [x_0,y_0,two_step[1],two_step[2],two_step[3],two_step[4]]
        
    return two_step_g_val,prod_diff_set,first_stage_second_stage_questions

--------------------------------------------------------------------------------------------------------------------------------------------------------------
*************************************************************************************************************************************************************
--------------------------------------------------------------------------------------------------------------------------------------------------------------

#A function which returns the best performing solution and their two-step acquisition values, along with the corresponding
#first stage and two second stage questions.
#We use the exact two-step g function. Used for experimenting and gaining insight into two-step acquisition.

def enum_two_step_opt(two_step_values,first_second_stage_question):
    #two_step_values: A list of two_step values. This should come from the function enum_two_step
    
    
    #Find min index of the two-step enumeration. Use these values to return the optimal two-step value,
    #along with their corresponding questions.
    two_step_array = np.array(two_step_values)
    min_index = np.argmin(two_step_array)
    max_index = np.argmax(two_step_array)
    opt_x0 = first_second_stage_question[min_index][0]
    opt_y0 = first_second_stage_question[min_index][1]
    opt_x10 = first_second_stage_question[min_index][2]
    opt_y10 = first_second_stage_question[min_index][3]
    opt_x11 = first_second_stage_question[min_index][4]
    opt_y11 = first_second_stage_question[min_index][5]
    opt_val = two_step_values[min_index]

    return [opt_val,opt_x0,opt_y0,opt_x10,opt_y10,opt_x11,opt_y11]
    
--------------------------------------------------------------------------------------------------------------------------------------------------------------
*************************************************************************************************************************************************************
--------------------------------------------------------------------------------------------------------------------------------------------------------------

#This code is used to get the quantile of our two-step approximate solution value relative to the optimal solution value
#as well as the one-step solution value relative to the optimal solution value. Also returns
#a dataframe containing all of the enumeration data from each replication

def quantile_test_enum_data(attr_num,prod_diff_list,rep_num,scale,location,mu_log_coeff,Sig_log_coeff,epsilon_0 = 0.1):
    #attr_num: the number of attributes the products have.
    
    #prod_diff_list: A list of product pairs we will enumerate over for each replication. 
    #Products should have same number of attributes as attr_num.
    
    #rep_num: the number of replications we wish to make
    
    #scale: A parameter to increase/decrease the determinant of the random covariance matrix
    
    #location: A parameter to increase/decrease the magnitude of the partworth estimator.
    
    #mu_log_coeff and Sig_log_coeff: linear coefficients that are used in the optimization problem
    
    #epsilon_0: A parameter that is used in the two-step optimization problem with orthogonality constraints. 
    #It sets the degree of orthogonality, and we want it to be small.
    
    
    #create two lists to store quantile data for one-step and two-step
    quant_val_one_step = [0.0]*rep_num
    quant_val_two_step = [0.0]*rep_num
    
    one_step_sol_val_list = [0.0]*rep_num
    two_step_sol_val_list = [0.0]*rep_num
    
    enum_df = pd.DataFrame()
    
    #Collect quantile data
    for i in range(0, rep_num):
        #prior parameters
        mu_0_rand = location*rng.uniform(low = -1.0, high = 1.0, size = attr_num) #use rng, this is Unif(-1.0,1.0)
        Sig_0_rand = scale*sklearn.datasets.make_spd_matrix(attr_num) #NEED SEED FOR THIS
        
        #Solve the two-step(with orthgonality constraint) and one-step optimization problems
        [w_0,z_0,w_1,z_1] = two_stage_g_opt(mu_0_rand,Sig_0_rand,mu_log_coeff,Sig_log_coeff,epsilon_0)
        [x_0, y_0] = g_opt(mu_0_rand,Sig_0_rand,mu_log_coeff,Sig_log_coeff)[1:]
        
        #Save the two-step acquisition function values of the one-step and two-step solutions
        one_step_sol_val = two_step_g_acq(mu_0_rand, Sig_0_rand,mu_log_coeff,Sig_log_coeff, x_0, y_0)[0]
        two_step_appr_sol_val_0 = two_step_g_acq(mu_0_rand, Sig_0_rand,mu_log_coeff,Sig_log_coeff, w_0,z_0)[0]
        two_step_appr_sol_val_1 = two_step_g_acq(mu_0_rand, Sig_0_rand,mu_log_coeff,Sig_log_coeff, w_1,z_1)[0]
        two_step_appr_sol_val = min(two_step_appr_sol_val_0,two_step_appr_sol_val_1)
        
        one_step_sol_val_list[i] = one_step_sol_val
        two_step_sol_val_list[i] = two_step_appr_sol_val
        
        #Create a list storing all of the enumerated values of all question pairs evaluated
        #under two-step acquisition function. Return a dataframe containing enumeration if needed.
        enum_data = enum_two_step(mu_0_rand,Sig_0_rand,mu_log_coeff,Sig_log_coeff,prod_diff_list)[0]
        enum_df['Trial: %s'%(str(i))] = enum_data
        
        #Calculate the quantile values of one-step and two-step. The best performing solution will have a quant
        #value of 1.0 and the worst performing solution will have a quant value of 0.
        enum_data_len = len(enum_data)
        one_step_quantile = [1 for x in enum_data if x >= one_step_sol_val]
        two_step_quantile = [1 for x in enum_data if x >= two_step_appr_sol_val]
        quant_val_one_step[i] = sum(one_step_quantile)/enum_data_len
        quant_val_two_step[i] = sum(two_step_quantile)/enum_data_len
     
    return [quant_val_one_step, quant_val_two_step, enum_df, one_step_sol_val_list, two_step_sol_val_list]

--------------------------------------------------------------------------------------------------------------------------------------------------------------
*************************************************************************************************************************************************************
--------------------------------------------------------------------------------------------------------------------------------------------------------------

#This function is used to compare one-step and two-step in a sequential fashion. We start the experiment with
#some prior information (expectation and covariance matrix), and sample a number of individuals
#(partworth vectors) from this prior. We then run the two questionnaires (one-step and two-step) on all of the individuals,
#recording the MSE between the estimator and their true partworth vector after each question, and we also record their MSE
#and determinant information before starting the questionnaire.
def MSE_det_test(attr_num,mu_scale,Sig_scale,num_rep,num_beta,noise_par,mu_log_coeff,Sig_log_coeff, use_max_min_mse_beta = True,
                 rand_samp_par = True, epsilon_0 = 0.1,question_num=16,r=0.5,t = 100):
    #attr_num: number of attributes the products should have.
    
    #mu_scale: used to create the expectation of the normal distribution we sample true partworth vectors from.
    
    #Sig_Scale: used to create the covariance matrix of the normal distribution we sample true partworth vectors from.
    
    #num_rep: The number of times we run the sequential experiment on ONE individual partworth vector.
    
    #num_beta: The number of individuals (partworth vectors) we sample for our experiment.
    
    #noise_par: This is a parameter which is used to increase the weight of the individuals' true partworth when making decision
    #between x and y. The higher this weight is, the less effect the gumbel random variable has on user choice.
    
    #mu_log_coeff and Sig_log_coeff: linear coefficients that are used in the optimization problem
    
    #use_approx_two_step: Determines whether we use the approximate two-step method or the true two-step method via enumeration
    
    #use_max_min_mse_beta: Determines if we only experiment using the two betas with max and min MSE compared to the
    #intial prior expectation.
    
    #rand_samp_par: This boolean parameter decides whether we use randomly generated expectation and 
    #covariance matrix or if we use
    #constant vector and diagonal covariance matrix for the sampling of true partworth vectors.
    
    #epsilon_0: This is used in the two-step optimization problem to set the degree of orthogonality between
    #the first-stage and second-stage question pairs
    
    #question_num: number of questions in the questionnaire.
    
    #r: coefficient for the KMS matrix, which acts as the prior covariance matrix.
    
    #t: this is the time limit on the approximate two-step method.
    
    
    #Use rng to generate random. SHOULD rng BE INSIDE THE FUNCTION?
    #rng = np.random.default_rng(0)
    
    #Decide whether we use randomly generated prior.
    if rand_samp_par:
        pop_prior_mu = mu_scale*rng.uniform(size = attr_num)
        pop_prior_Sig = Sig_scale*sklearn.datasets.make_spd_matrix(attr_num)
    else:
        pop_prior_mu = np.array(attr_num*[mu_scale])
        pop_prior_Sig = Sig_scale*np.identity(attr_num)
    
    #Used to store partworth vectors
    true_betas = []

    #Sample from prior distribution to get partworths
    for i in range(0,num_beta):
        true_betas.append(rng.multivariate_normal(pop_prior_mu,pop_prior_Sig))
    
    #Start the experiment
    mu_start = pop_prior_mu #rng.uniform(low = -0.1, high = 0.1, size = attr_num)#mu_scale*np.array(attr_num*[0.01])
    Sig_start = pop_prior_Sig #Sig_scale*KMS_Matrix(attr_num,r)
    
    #If use_max_min_mse_beta is true, then we only use the betas with the smallest and largest MSE from the experiment.
    #MSE(true_beta/||true_beta||, initial_est/||initial_est||)
    if use_max_min_mse_beta:
        init_mse_values = num_beta*[0.0]
        for i in range(0,num_beta):
            init_mse_values[i] =  np.square(np.subtract(true_betas[i]/np.linalg.norm(true_betas[i],ord = 2),
                                                            mu_start/np.linalg.norm(mu_start, ord = 2))).mean()
        init_mse_max_index = np.argmax(np.array(init_mse_values))
        init_mse_min_index = np.argmin(np.array(init_mse_values))
        true_betas = [true_betas[init_mse_min_index],true_betas[init_mse_max_index]]
    
    beta_len = len(true_betas)
    
    #These will be used to store MSE and covariance matrix determinant values after each question is asked.
    one_step_det = [[[] for j in range(beta_len)] for i in range(question_num + 1)]
    one_step_mse = [[[] for j in range(beta_len)] for i in range(question_num + 1)]
    
    true_two_step_det = [[[] for j in range(beta_len)] for i in range(question_num + 1)]
    true_two_step_mse = [[[] for j in range(beta_len)] for i in range(question_num + 1)]
    
    appr_two_step_det = [[[] for j in range(beta_len)] for i in range(question_num + 1)]
    appr_two_step_mse = [[[] for j in range(beta_len)] for i in range(question_num + 1)]
    
    
    for b in range(0,beta_len):
        for i in range(0,num_rep):
            #Start each individual with the same prior information for both one-step and two-step methods. Store
            #starting determinant value and MSE
            one_step_mu = mu_start
            one_step_Sig = Sig_start
            true_two_step_mu = mu_start
            true_two_step_Sig = Sig_start
            appr_two_step_mu = mu_start
            appr_two_step_Sig = Sig_start
            #redundant
            one_step_det[0][b].append(np.linalg.det(Sig_start))
            true_two_step_det[0][b].append(np.linalg.det(Sig_start))
            appr_two_step_det[0][b].append(np.linalg.det(Sig_start))

            one_step_mse[0][b].append(np.square(np.subtract(true_betas[b]/np.linalg.norm(true_betas[b],ord = 2),
                                                            one_step_mu/np.linalg.norm(one_step_mu, ord = 2))).mean())
            true_two_step_mse[0][b].append(np.square(np.subtract(true_betas[b]/np.linalg.norm(true_betas[b],ord = 2),
                                                            true_two_step_mu/np.linalg.norm(true_two_step_mu, ord = 2))).mean())
            appr_two_step_mse[0][b].append(np.square(np.subtract(true_betas[b]/np.linalg.norm(true_betas[b],ord = 2),
                                                            appr_two_step_mu/np.linalg.norm(appr_two_step_mu, ord = 2))).mean())
            for j in range(1,question_num + 1):
                #get questions for one step
                [one_step_x,one_step_y] = g_opt(one_step_mu,one_step_Sig,mu_log_coeff,Sig_log_coeff)[1:]
                
                #get questions for approx two step
                [w_0,z_0,w_1,z_1] = two_stage_g_opt(appr_two_step_mu,appr_two_step_Sig,mu_log_coeff,Sig_log_coeff,epsilon_0,t_lim=t)
                two_step_appr_sol_val_0 = two_step_g_acq(appr_two_step_mu, appr_two_step_Sig,mu_log_coeff,Sig_log_coeff, w_0,z_0)
                two_step_appr_sol_val_1 = two_step_g_acq(appr_two_step_mu, appr_two_step_Sig,mu_log_coeff,Sig_log_coeff, w_1,z_1)
                if two_step_appr_sol_val_0 < two_step_appr_sol_val_1:
                    appr_two_step_x = w_0
                    appr_two_step_y = z_0
                else:
                    appr_two_step_x = w_1
                    appr_two_step_y = z_1
            
                #List of question pairs we will enumerate on for true two-step
                prod_list_enum_two_step = product_diff_list(attr_num)

                #Do the enumeration procedure
                enumerated_solution_list_two_step = enum_two_step(true_two_step_mu,true_two_step_Sig,mu_log_coeff,
                                                             Sig_log_coeff,prod_list_enum_two_step)
                    
                opt_sol_questions_two_step = enum_two_step_opt(enumerated_solution_list_two_step[0],
                                                          enumerated_solution_list_two_step[2])
                    
                #get the first stage products for true two-step
                true_two_step_x = np.array(opt_sol_questions_two_step[1])
                true_two_step_y = np.array(opt_sol_questions_two_step[2])
                    

                gum_x = rng.gumbel(0,1,1)
                gum_y = rng.gumbel(0,1,1)

                #These temp variables will be used in the choice model below in case the user prefers y over x.
                one_step_x_temp = one_step_x
                one_step_y_temp = one_step_y
                true_two_step_x_temp = true_two_step_x
                true_two_step_y_temp = true_two_step_y
                appr_two_step_x_temp = appr_two_step_x
                appr_two_step_y_temp = appr_two_step_y

                #See preference between two products
                #set signal to noise ratio
                if (noise_par*np.dot(true_betas[b],np.array(one_step_y)) + gum_y) >= (noise_par*np.dot(true_betas[b],np.array(one_step_x))
                                                                               + gum_x):
                    one_step_x = one_step_y_temp
                    one_step_y = one_step_x_temp
                if (noise_par*np.dot(true_betas[b],np.array(true_two_step_y)) + gum_y) >= (noise_par*np.dot(true_betas[b],np.array(true_two_step_x))
                                                                               + gum_x):
                    true_two_step_x = true_two_step_y_temp
                    true_two_step_y = true_two_step_x_temp
                if (noise_par*np.dot(true_betas[b],np.array(appr_two_step_y)) + gum_y) >= (noise_par*np.dot(true_betas[b],np.array(appr_two_step_x))
                                                                               + gum_x):
                    appr_two_step_x = appr_two_step_y_temp
                    appr_two_step_y = appr_two_step_x_temp
            

                #Perform moment matching after choice is made.
                [one_step_mu, one_step_Sig] = moment_matching_update(one_step_x,one_step_y,one_step_mu,one_step_Sig)
                [true_two_step_mu, true_two_step_Sig] = moment_matching_update(true_two_step_x,true_two_step_y,true_two_step_mu,true_two_step_Sig)
                [appr_two_step_mu, appr_two_step_Sig] = moment_matching_update(appr_two_step_x,appr_two_step_y,appr_two_step_mu,appr_two_step_Sig)


                #Compute determinant and MSE after question j, save information in a list corresponding to question j.
                #This list will hold all of the MSE and determinant information for all individuals for question j.
                one_step_det[j][b].append(np.linalg.det(one_step_Sig))
                true_two_step_det[j][b].append(np.linalg.det(true_two_step_Sig))
                appr_two_step_det[j][b].append(np.linalg.det(appr_two_step_Sig))
                
                one_step_mse[j][b].append(np.square(np.subtract(true_betas[b]/np.linalg.norm(true_betas[b],ord = 2),
                                                            one_step_mu/np.linalg.norm(one_step_mu, ord = 2))).mean())
                true_two_step_mse[j][b].append(np.square(np.subtract(true_betas[b]/np.linalg.norm(true_betas[b],ord = 2),
                                                            true_two_step_mu/np.linalg.norm(true_two_step_mu, ord = 2))).mean())
                appr_two_step_mse[j][b].append(np.square(np.subtract(true_betas[b]/np.linalg.norm(true_betas[b],ord = 2),
                                                            appr_two_step_mu/np.linalg.norm(appr_two_step_mu, ord = 2))).mean())
            
    return [one_step_det,one_step_mse,true_two_step_det,true_two_step_mse,appr_two_step_det,appr_two_step_mse]


--------------------------------------------------------------------------------------------------------------------------------------------------------------
*************************************************************************************************************************************************************
--------------------------------------------------------------------------------------------------------------------------------------------------------------

#!!!THIS FUNCTION IS A REWRITE OF MSE_det_test!!!#
#This function is used to compare different methods in a sequential fashion using MSE and determinant as a measurement 
#of quality. In this function, we only use one acquisition method at a time (compare with MSE_det_test).

def new_mse_det_hitrate_experiment(init_mu, init_Sig, true_partworths, rep_per_partworth, num_questions, mu_log_coeff, Sig_log_coeff,
                          noise_par, hitrate_question_list, Method = 0, batch_size = 0, MC_budget = 0, include_one_step = False):
    #init_mu: This is the initial estimate on the partworths
    #init_Sig: This is the initial covariance matrix on the partworths
    #true_partworths: These are used to make selection in the product selection stage (a list/set of partworths)
    #rep_per_partworth: This is the number of times we want to conduct a questionnaire on each partworth
    #num_questions: Length of the questionnaire
    #mu_log_coeff; Sig_log_coeff: These are the coefficients in the optimization model for the linearized objective function
    #noise_par: This is a parameter which is used to increase the weight of the individuals' true partworth when making decision
    #between x and y. The higher this weight is, the less effect the gumbel random variable has on user choice.
    #hitrate_question_list: This is a list of questions which will be used to calculate the hitrate, which is the
    #proportion of times that an estimated partworth matches the product selection of a true underlying partworth.
    #Method: 0 - One step look ahead
    #        1 - Rollout with batch design   <------ CAN ADD MORE METHODS IF NEEDED
    #batch_size: Used to select batch size if we use rollout
    #MC_budget: Used to select Monte Carlo budget if we use rollout.
    #include_one_step: This determines whether we want to include the one-step optimal question within our batch. This can
    #help ensure that rollout performs at least as well as one-step look ahead. Default value is False.
    
    #Construct a list for storing MSE information. For each of the baseline partworths, we create a list holding
    #(num_questions) lists, where each of the (num_questions) lists holds the MSE values for all replications
    #after the first, second,...,nth question
    
    num_true_partworth = len(true_partworths)
    
    MSE = [[[] for j in range(num_questions)] for u in range(num_true_partworth)]
    
    DET = [[[] for j in range(num_questions)] for u in range(num_true_partworth)]
    
    for u in range(num_true_partworth):
        for i in range(rep_per_partworth):
            #Instantiate mu and Sig with the initial parameters init_mu and init_Sig. These act as prior parameters for all
            #the partworths
            mu = init_mu
            Sig = init_Sig
            
            for j in range(num_questions):
                if Method == 0:
                    #get optimal question for one step
                    [x,y] = g_opt(mu,Sig,mu_log_coeff,Sig_log_coeff)[1:]
                    
                if Method == 1:
                    #get optimal question for rollout over batch design
                    #Set rollout_len so that it uses the budget of questions we have. If we have a budget of 16 questions,
                    #we will want to first start out by doing a rollout of length 16, then 15, then 14... A rollout length
                    #of 0 means we only look at the one-step lookahead values of the questions in the batch.
                    rollout_len = num_questions - j - 1
                    [x,y] = rollout_with_batch_design_acquisition(mu,Sig,mu_log_coeff,Sig_log_coeff,batch_size,
                                                                 rollout_len,MC_budget,include_one_step)
                    
                #Instantiate gumbel random variables which are used in the product choice selection process.
                gum_x = rng.gumbel(0,1,1)
                gum_y = rng.gumbel(0,1,1)
                    
                #These temp variables will be used in the choice model below in case the user prefers y over x.
                x_temp = x
                y_temp = y
                    
                #See preference between two products
                #set signal to noise ratio
                if (noise_par*np.dot(true_partworths[u],np.array(y)) + gum_y) >= (noise_par*np.dot(true_partworths[u],np.array(x))
                                                                               + gum_x):
                    x = y_temp
                    y = x_temp
                
                #Perform moment matching after choice is made.
                [mu, Sig] = moment_matching_update(x,y,mu,Sig)
                
                #Add the MSE between the true partworth and estimator at question j to a list, and add the determinant of
                #the covariance matrix at question j into a list
                MSE[u][j].append(np.square(np.subtract(true_partworths[u]/np.linalg.norm(true_partworths[u],ord = 2),
                                                            mu/np.linalg.norm(mu, ord = 2))).mean())
                DET[u][j].append(np.linalg.det(Sig))
                
            #Calculate hitrate for this replication
            hits = 0
            for q in hitrate_question_list:
                if np.dot(true_partworths[u],q)*np.dot(mu,q)>=0:
                    hits = hits + 1
            HITRATE[u].append(hits/hitrate_total_num_of_questions)
    
    return[MSE,DET,HITRATE]
